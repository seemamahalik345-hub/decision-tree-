{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " 1. What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "  - A Decision Tree is a supervised machine learning algorithm used for classification and regression. In classification, it predicts a class label by learning a sequence of if–then rules from the training data."
      ],
      "metadata": {
        "id": "tiAS6k47MrPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "  - In decision tree classification, impurity measures quantify how mixed the class labels are in a node. The goal at each split is to reduce impurity as much as possible, creating purer child nodes.\n"
      ],
      "metadata": {
        "id": "qUhXeEtYNBPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "  - Both pre-pruning and post-pruning are techniques used to control overfitting in decision trees by limiting model complexity, but they differ in when pruning is applied"
      ],
      "metadata": {
        "id": "rtS4FNlrNl1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "  - Information Gain (IG) is a criterion used in decision tree classification to decide which feature and split should be chosen at each node. It measures how much uncertainty (impurity) is reduced after a dataset is split on a particular feature."
      ],
      "metadata": {
        "id": "IsBSWxHsOHqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "  - Decision Trees are widely used because they are simple, interpretable, and effective for both classification and regression tasks."
      ],
      "metadata": {
        "id": "apR66_fHOY4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "nOz5PbOpOq-S"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cf7a7ed",
        "outputId": "d6b1c234-753d-433b-9a4d-a87c3a977041"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Fully-grown Decision Tree Classifier (criterion='gini') ---\n",
        "decision_tree_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "decision_tree_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set for the fully-grown tree\n",
        "y_pred_full = decision_tree_full.predict(X_test)\n",
        "\n",
        "# Print the model's accuracy for the fully-grown tree\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Fully-grown Decision Tree Accuracy: {accuracy_full:.4f}\")\n",
        "\n",
        "# --- Decision Tree Classifier with max_depth=3 (criterion='gini') ---\n",
        "decision_tree_limited = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "decision_tree_limited.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set for the limited depth tree\n",
        "y_pred_limited = decision_tree_limited.predict(X_test)\n",
        "\n",
        "# Print the model's accuracy for the limited depth tree\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "print(f\"Decision Tree (max_depth=3) Accuracy: {accuracy_limited:.4f}\")\n",
        "\n",
        "# Print feature importances for the fully-grown tree (as it was in the original request)\n",
        "print(\"\\nFeature Importances (Fully-grown tree):\")\n",
        "for feature, importance in zip(iris.feature_names, decision_tree_full.feature_importances_):\n",
        "    print(f\"  {feature}: {importance:.4f}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown Decision Tree Accuracy: 1.0000\n",
            "Decision Tree (max_depth=3) Accuracy: 1.0000\n",
            "\n",
            "Feature Importances (Fully-grown tree):\n",
            "  sepal length (cm): 0.0000\n",
            "  sepal width (cm): 0.0191\n",
            "  petal length (cm): 0.8933\n",
            "  petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1N_uOuYgRuSY"
      }
    }
  ]
}